## Tutorial-type notebooks covering regression, classification, clustering, dimensionality reduction, and some basic neural network algorithms

### Regression
* Simple linear regression with t-statistic generation
<img src="https://slideplayer.com/slide/6053182/20/images/10/Simple+Linear+Regression+Model.jpg" width="400" height="300"/>

* [Multiple ways to perform linear regression in Python and their speed comparison](Regression/Linear_Regression_Methods.ipynb) ([check the article I wrote on freeCodeCamp](https://medium.freecodecamp.org/data-science-with-python-8-ways-to-do-linear-regression-and-measure-their-speed-b5577d75f8b))

* [Multi-variate regression with regularization](Regression/Multi-variate%20LASSO%20regression%20with%20CV.ipynb)
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/L1_and_L2_balls.svg/300px-L1_and_L2_balls.svg.png"/>

* Polynomial regression using ***scikit-learn pipeline feature*** ([check the article I wrote on *Towards Data Science*](https://towardsdatascience.com/machine-learning-with-python-easy-and-robust-method-to-fit-nonlinear-data-19e8a1ddbd49))

* [Decision trees and Random Forest regression](Regression/Random_Forest_Regression.ipynb) (showing how the Random Forest works as a robust/regularized meta-estimator rejecting overfitting)

* [Detailed visual analytics and goodness-of-fit diagnostic tests for a linear regression problem](Regression/Regression_Diagnostics.ipynb)

* [Robust linear regression using `HuberRegressor` from Scikit-learn](Regression/Robust%20Linear%20Regression.ipynb)

-----

### Classification
* Logistic regression/classification ([Here is the Notebook](Classification/Logistic_Regression_Classification.ipynb))
<img src="https://qph.fs.quoracdn.net/main-qimg-914b29e777e78b44b67246b66a4d6d71"/>

* _k_-nearest neighbor classification ([Here is the Notebook](Classification/KNN_Classification.ipynb))

* Decision trees and Random Forest Classification ([Here is the Notebook](Classification/DecisionTrees_RandomForest_Classification.ipynb))

* Support vector machine classification ([Here is the Notebook](Classification/Support_Vector_Machine_Classification.ipynb)) (**[check the article I wrote in Towards Data Science on SVM and sorting algorithm](https://towardsdatascience.com/how-the-good-old-sorting-algorithm-helps-a-great-machine-learning-technique-9e744020254b))**

<img src="https://docs.opencv.org/2.4/_images/optimal-hyperplane.png"/>

* Naive Bayes classification ([Here is the Notebook](Classification/Naive_Bayes_Classification.ipynb))

---

### Clustering
<img src="https://i.ytimg.com/vi/IJt62uaZR-M/maxresdefault.jpg" width="450" height="300"/>

* _K_-means clustering ([Here is the Notebook](Clustering-Dimensionality-Reduction/K_Means_Clustering_Practice.ipynb))

* Affinity propagation (showing its time complexity and the effect of damping factor) ([Here is the Notebook](Clustering-Dimensionality-Reduction/Affinity_Propagation.ipynb))

* Mean-shift technique (showing its time complexity and the effect of noise on cluster discovery) ([Here is the Notebook](Clustering-Dimensionality-Reduction/Mean_Shift_Clustering.ipynb))

* DBSCAN (showing how it can generically detect areas of high density irrespective of cluster shapes, which the k-means fails to do) ([Here is the Notebook](Clustering-Dimensionality-Reduction/DBScan_Clustering.ipynb))

* Hierarchical clustering with Dendograms showing how to choose optimal number of clusters ([Here is the Notebook](Clustering-Dimensionality-Reduction/Hierarchical_Clustering.ipynb))

---

### Hiden Markovchain

* Hiden Markovchain ([Here is the Notebook](HMM/Hidden%20Markov%20Models%20Tutorial.ipynb))

### Dimensionality reduction
* Principal component analysis

<img src="https://i.ytimg.com/vi/QP43Iy-QQWY/maxresdefault.jpg" width="450" height="300"/>

---